##聚类数据

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.cluster import KMeans
#%matplotlib inline

##转换成pickle文件
#rtm=pd.read_csv(r'C:\Users\dell\Desktop\data_deep\ws\dataset2\newdata\New_rtdata.csv',header=None)
##tpm=pd.read_csv(r'C:\Users\dell\Desktop\data_deep\ws\dataset2\newdata\New_tpdata.csv',header=None)
#
###删除第一行第一列
#rtm1 = np.array(rtm.values[1:4501,1:9089],dtype=float)#只选有数的部分
#rtm1 = pd.DataFrame(rtm1)
#rtm1.to_pickle(r'C:\Users\dell\Desktop\data_deep\ws\dataset2\newdata\New_rtdata.pickle') #save results as csv - this file is ready for SVM in R ###CHANGE HERE#####

###读取原始数据 
#rtm=pd.read_pickle(r'C:\Users\18742\Desktop\data_deep\ws\dataset2\newdata\New_rtdata.pickle')
rtm=pd.read_pickle(r'C:\Users\dell\Desktop\data_deep\ws\dataset2\newdata\New_tpdata.pickle')

#rtm = np.array(rtm.values[:,1:],dtype=float)
#rtm = pd.DataFrame(rtm)
#rtm.to_pickle(r'C:\Users\dell\Desktop\data_deep\ws\dataset2\newdata\New_tpdata.pickle')
#求均值
#m = np.sum(rtm)
#n = np.sum(m)
#a = n/4500
#a = a/9088
data1=pd.read_pickle(r'C:\Users\dell\Desktop\data_deep\ws\dataset2\newdata\rt-tpdata-new-4500\rt-tpdata-new-4500gm.pickle')
rtm = np.array(data1.values[:,:-1],dtype=float)

##对数据进行标准化处理
min_max_scaler = preprocessing.MinMaxScaler()
rtm_pre = min_max_scaler.fit_transform(rtm)
rtm_pre = pd.DataFrame(rtm_pre)


#设置随机种子
random_state=20

##确定聚类的个数
SSE = []#存放每次结果的误差平方和
for k in range(1, 10):
    estimator = KMeans(n_clusters=k)  # 构造聚类器
    estimator.fit(np.array(rtm_pre.values[0:4500,0:18176]))##total_df[total_df.columns[:-1]].values
    SSE.append(estimator.inertia_)
X = range(1, 10)
plt.xlabel('k')
plt.ylabel('SSE')
plt.plot(X, SSE, 'o-')
plt.show()


#########################################
##tsne降维
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
#%matplotlib qt
kmeans = KMeans(n_clusters=4, random_state=random_state).fit(rtm_pre)
tsne = TSNE(n_components=2, learning_rate=100).fit_transform(rtm_pre)
pca = PCA().fit_transform(rtm_pre)
label = kmeans.labels_ #得到的聚类结果
#y_pred = kmeans.fit_predict(rtm_pre) #两种方式都是预测的聚类结果
center = kmeans.cluster_centers_ #簇的中心点
#plt.scatter(tsne[:,0], tsne[:,1], c=label)############
#plt.colorbar()
#plt.title("kmeans")

plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.scatter(tsne[:, 0], tsne[:, 1], c=label)
plt.subplot(122)
plt.scatter(pca[:, 0], pca[:, 1], c=label)
plt.colorbar()
plt.show()

rtm_pre["label"] = label
#rtm_pre["label"] = y_pred
rtm_pre.to_pickle(r'C:\Users\dell\Desktop\data_deep\ws\dataset2\newdata\rtm_predy.pickle') #save results as csv - this file is ready for SVM in R ###CHANGE HERE#####
#rtm_pre.to_pickle(r'C:\Users\dell\Desktop\data_deep\ws\dataset2\newdata\rtm_predyclass2.pickle') #save results as csv - this file is ready for SVM in R ###CHANGE HERE#####

####单独保存sid和label数据 对每一种分类结果，后面就可以直接使用分类结果的数据按照sid顺序加进去就可以了
sid_label2 = pd.DataFrame()
sid_label2['sid'] = data1.sid
sid_label2['label'] = label
sid_label2.to_pickle(r"C:\Users\dell\Desktop\data_deep\ws\dataset2\newdata\sid_label\sid_label2.pickle")
#sid_label5.to_pickle(r"C:\Users\dell\Desktop\data_deep\ws\dataset2\newdata\sid_label\sid_label5.pickle")

##再次读取数据的时候就分别读取数据，把label和进去就可以
#对于5类的数据，0,1,2,3,4个label真实值，以及预测的值0,1,2,3,4


####################################最简单的聚类，可以对y进行预测，
##但是不能可视化，因为没有实现降维
#column=list(rtm.columns)
##分类标准是什么？x是所有的特征，y是分类的结果可以进行设置,分成四类查看效果,对sid的聚类
#X = rtm#应该将所有的特征包括，应该设置一个新的评价标准，例如score一样的
#
#kmeans = KMeans(n_clusters=5, random_state=random_state).fit(X)
##kmeans = KMeans(n_clusters=3, random_state=random_state).fit(X)
#
#label = kmeans.labels_ #得到的聚类结果
#y_pred = kmeans.fit_predict(X) #两种方式都是预测的聚类结果
#center = kmeans.cluster_centers_ #簇的中心点
#X["cluster"] = kmeans.labels_
##%matplotlib inline
##plt.scatter(X[0], X[1],c=X["cluster"])
#
##print('kmeans:', kmeans)
##print('labels:', label)
##print('y_pred:',y_pred)
##print('centers:', center)
#
#plt.scatter(X[0], X[1], c=label)
#plt.colorbar()
#plt.title("kmeans")

####绘制热图（无树状图的）
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
sns.set()
ax = sns.heatmap(rtm_pre)
plt.show()


####有树状图的
g= sns.clustermap(rtm_pre)
ax = g.ax_heatmap
label_y = ax.get_yticklabels()
plt.setp(label_y, rotation=360, horizontalalignment='left')
plt.show()


#观察每一行数据，画出质谱图**因为存在20相应时间过长的情况
##同一服务不同用户差别较大
from matplotlib.pyplot import plot, legend, show,figure,hold
row = rtm_pre[898][:64]
row_ = rtm_pre[898][64:128]
row_ = pd.DataFrame(row_)
row_2 = row_.reset_index(drop=True)##将索引重新进行排列从0开始
figure(figsize=(15,15))
plot(row, linewidth=1.75, alpha=0.75)
hold(figure)
plot(row_2,linewidth=1.75, alpha=0.75)
legend('l')
show()

###同一用户不同服务比较接近
#row = rtm_pre[2000][:64]
#row_2 = rtm_pre[989][:64]
#figure(figsize=(15,15))
#plot(row, linewidth=1.75, alpha=0.75)
#hold(figure)
#plot(row_2,linewidth=1.75, alpha=0.75)
#legend('l')
#show()


###这里显示的是相同类型的服务直接的rt是区别不大的，趋势是差不多的，所以使用dtw进行分类更为合理
##每隔相同的时间间隔的同一用户对应的数据


###DBCAN   结果不好，按照这个数据分布的情况，使用密度聚类的结果并不能将其分成合适的类别
from sklearn.cluster import DBSCAN  
#random_state = 2020
#db = DBSCAN().fit(rtm_pre)##需要调参
db = DBSCAN(eps=30,min_samples=50).fit(rtm_pre)
tsne = TSNE(n_components=2, learning_rate=100).fit_transform(rtm_pre)
pca = PCA().fit_transform(rtm_pre)
label = db.labels_
#y_pred = db.fit_predict(rtm_pre) #两种方式都是预测的聚类结果和label

group_labels = set(label) #标签种类
n_clusters_ = len(group_labels) - (1 if -1 in label else 0) #若有噪音点，将噪音点减去得到分簇的个数
n_noise_ = list(label).count(-1)
print('dbscan:', db)
print('labels:', label)
print('group_labels:', group_labels)
print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)


######选择eps的方法
# 构建空列表，用于保存不同参数组合下的结果
res = []
# 迭代不同的eps值
for eps in np.arange(0.001,1,0.05):
    # 迭代不同的min_samples值
    for min_samples in range(2,10):
        dbscan = cluster.DBSCAN(eps = eps, min_samples = min_samples)
        # 模型拟合
        dbscan.fit(X)
        # 统计各参数组合下的聚类个数（-1表示异常点）
        n_clusters = len([i for i in set(dbscan.labels_) if i != -1])
        # 异常点的个数
        outliners = np.sum(np.where(dbscan.labels_ == -1, 1,0))
        # 统计每个簇的样本个数
        stats = str(pd.Series([i for i in dbscan.labels_ if i != -1]).value_counts().values)
        res.append({'eps':eps,'min_samples':min_samples,'n_clusters':n_clusters,'outliners':outliners,'stats':stats})
# 将迭代后的结果存储到数据框中        
df = pd.DataFrame(res)

# 根据条件筛选合理的参数组合
df.loc[df.n_clusters == 3, :]


##用不同的图标表示的方式
#n_clusters = len(set(label)) 
#markers = ['^','x','o','*','+']  
#for i in range(n_clusters):    
#    plt.scatter(tsne[:, 0], tsne[:, 1],s=60,marker=markers[i],c=label,alpha=0.5)  
#plt.title("dbscan_tsne")
#plt.show() 
# 
#for i in range(n_clusters):    
#    plt.scatter(pca[:, 0], pca[:, 1],s=60,marker=markers[i],c=label,alpha=0.5)  
#plt.title("dbscan_pca")
#plt.show() 

###用不同颜色的表示方式
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.scatter(tsne[:, 0], tsne[:, 1], c=label)
plt.title("dbscan_tsne")
plt.subplot(122)
plt.scatter(pca[:, 0], pca[:, 1], c=label)
plt.title("dbscan_pca")
plt.colorbar()
plt.show()

#去除噪音点：对于属性label，删除掉取值为-1的那些样本。
label[label!=-1]
rtm_pre[pd.Series(label)!=-1]

rtm_pre.to_pickle(r'C:\Users\18742\Desktop\data_deep\ws\dataset2\newdata\rtmdbscan_pre_rtdata.pickle') #save results as csv - this file is ready for SVM in R ###CHANGE HERE#####
